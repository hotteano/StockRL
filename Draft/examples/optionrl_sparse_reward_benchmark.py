"""
OptionRL vs Q-Learning: ç¨€ç–å¥–åŠ±åŸºå‡†æµ‹è¯•

æœ¬è„šæœ¬åœ¨å¤šä¸ªç¨€ç–å¥–åŠ±ç¯å¢ƒä¸Šæ¯”è¾ƒ:
1. FrozenLake 4x4 (ç®€å•åŸºå‡†)
2. FrozenLake 8x8 (æ›´å¤§çŠ¶æ€ç©ºé—´ã€æ›´é•¿è·¯å¾„)
3. Taxi-v3 (500 çŠ¶æ€ã€é•¿åºåˆ—ã€ç¨€ç–å¤§ç»ˆå€¼)

æ ¸å¿ƒå‡è®¾: OptionRL é€šè¿‡ä¼°è®¡"é£é™©ä¸­æ€§æµ‹åº¦ä¸‹çš„ç»ˆå€¼æœŸæœ›"C_tï¼Œ
åœ¨ç¨€ç–å¥–åŠ±ä»»åŠ¡ä¸­æ¯”ä¼ ç»Ÿ Q-learning æ›´é«˜æ•ˆåœ°ä¼ æ’­è¿œæœŸä»·å€¼ä¿¡å·ã€‚
"""

import math
import random
import time
from dataclasses import dataclass
from typing import Callable, Tuple

import gymnasium as gym
import numpy as np


# =============================================================================
# ç¯å¢ƒé…ç½®
# =============================================================================

@dataclass
class EnvConfig:
    env_id: str
    max_steps: int
    goal_reward: float  # æˆåŠŸæ—¶çš„å¥–åŠ±å€¼ï¼Œç”¨äºåˆ¤æ–­æ˜¯å¦æˆåŠŸ
    name: str           # æ˜¾ç¤ºåç§°


ENVS = {
    "frozenlake_4x4": EnvConfig(
        env_id="FrozenLake-v1",
        max_steps=100,
        goal_reward=1.0,
        name="FrozenLake 4x4 (slippery)",
    ),
    "frozenlake_8x8": EnvConfig(
        env_id="FrozenLake-v1",
        max_steps=200,
        goal_reward=1.0,
        name="FrozenLake 8x8 (slippery)",
    ),
    "taxi": EnvConfig(
        env_id="Taxi-v3",
        max_steps=200,
        goal_reward=20.0,
        name="Taxi-v3",
    ),
}


def make_env(cfg: EnvConfig):
    if "FrozenLake" in cfg.env_id:
        map_name = "8x8" if "8x8" in cfg.name else "4x4"
        return gym.make(cfg.env_id, map_name=map_name, is_slippery=True)
    return gym.make(cfg.env_id)


# =============================================================================
# è¾…åŠ©å‡½æ•°
# =============================================================================

def greedy_epsilon_policy(eps: float) -> Callable[[int, np.ndarray], int]:
    def policy(s: int, Q: np.ndarray) -> int:
        if random.random() < eps:
            return random.randint(0, Q.shape[1] - 1)
        return int(np.argmax(Q[s]))
    return policy


def estimate_goal_prob(
    mc_env,
    start_state: int,
    remaining_steps: int,
    policy: Callable[[int, np.ndarray], int],
    Q: np.ndarray,
    goal_reward: float,
    n_rollouts: int = 16,
) -> float:
    """
    ç”¨ Monte Carlo ä¼°è®¡ä» start_state å‡ºå‘ï¼Œ
    åœ¨ remaining_steps æ­¥å†…è·å¾— goal_reward çš„æ¦‚ç‡ã€‚
    """
    if remaining_steps <= 0:
        return 0.0

    success = 0
    for _ in range(n_rollouts):
        obs, _ = mc_env.reset()
        # å¼ºåˆ¶è®¾ç½®èµ·å§‹çŠ¶æ€
        mc_env.unwrapped.s = start_state
        done = False
        steps_left = remaining_steps
        final_reward = 0.0
        while not done and steps_left > 0:
            s = mc_env.unwrapped.s
            a = policy(s, Q)
            obs, r, terminated, truncated, _ = mc_env.step(a)
            done = terminated or truncated
            steps_left -= 1
            if done:
                final_reward = r
        # åˆ¤æ–­æ˜¯å¦æˆåŠŸåˆ°è¾¾ç›®æ ‡
        if final_reward >= goal_reward:
            success += 1
    return success / n_rollouts


def estimate_goal_prob_cached(
    mc_env,
    cache: dict,
    start_state: int,
    remaining_steps: int,
    policy: Callable[[int, np.ndarray], int],
    Q: np.ndarray,
    goal_reward: float,
    n_rollouts: int = 16,
) -> float:
    """å¸¦ç¼“å­˜çš„ç‰ˆæœ¬ï¼Œé¿å…åŒä¸€ (state, remaining_steps) åå¤ MCã€‚"""
    if remaining_steps <= 0:
        return 0.0
    key = (start_state, remaining_steps)
    if key in cache:
        return cache[key]
    prob = estimate_goal_prob(mc_env, start_state, remaining_steps, policy, Q, goal_reward, n_rollouts)
    cache[key] = prob
    return prob


# =============================================================================
# Q-Learning (æ—¶é—´é¢„ç®—ç‰ˆ)
# =============================================================================

def run_q_learning(
    cfg: EnvConfig,
    time_budget_s: float = 10.0,
    alpha: float = 0.1,
    gamma: float = 0.99,
    eps: float = 0.1,
) -> Tuple[np.ndarray, int, float]:
    """åœ¨ç»™å®šæ—¶é—´é¢„ç®—å†…è¿è¡Œ Q-learningã€‚"""
    env = make_env(cfg)
    n_states = env.observation_space.n
    n_actions = env.action_space.n
    Q = np.zeros((n_states, n_actions), dtype=float)
    returns = []
    successes = []

    start = time.time()
    episodes = 0
    while time.time() - start < time_budget_s:
        obs, _ = env.reset()
        s = int(obs)
        done = False
        ep_ret = 0.0
        steps = 0
        while not done and steps < cfg.max_steps:
            if random.random() < eps:
                a = random.randint(0, n_actions - 1)
            else:
                a = int(np.argmax(Q[s]))

            obs_next, r, terminated, truncated, _ = env.step(a)
            s_next = int(obs_next)
            done = terminated or truncated
            ep_ret += r

            td_target = r + (0.0 if done else gamma * np.max(Q[s_next]))
            Q[s, a] += alpha * (td_target - Q[s, a])

            s = s_next
            steps += 1

        returns.append(ep_ret)
        successes.append(1.0 if ep_ret >= cfg.goal_reward else 0.0)
        episodes += 1

    elapsed = time.time() - start
    env.close()
    return np.array(returns), np.array(successes), episodes, elapsed


# =============================================================================
# OptionRL-style Q-Learning (æ—¶é—´é¢„ç®—ç‰ˆ)
# =============================================================================

def run_optionrl(
    cfg: EnvConfig,
    time_budget_s: float = 10.0,
    alpha: float = 0.1,
    r_rate: float = 0.05,
    eps: float = 0.1,
    n_rollouts: int = 16,
) -> Tuple[np.ndarray, int, float]:
    """åœ¨ç»™å®šæ—¶é—´é¢„ç®—å†…è¿è¡Œ OptionRL-style Q-learningã€‚"""
    env = make_env(cfg)
    mc_env = make_env(cfg)
    n_states = env.observation_space.n
    n_actions = env.action_space.n
    Q = np.zeros((n_states, n_actions), dtype=float)
    returns = []
    successes = []

    disc = math.exp(-r_rate)
    policy_fn = greedy_epsilon_policy(eps)

    start = time.time()
    episodes = 0
    while time.time() - start < time_budget_s:
        cache: dict = {}
        obs, _ = env.reset()
        s = int(obs)
        done = False
        ep_ret = 0.0
        t = 0
        while not done and t < cfg.max_steps:
            a = policy_fn(s, Q)
            obs_next, r, terminated, truncated, _ = env.step(a)
            s_next = int(obs_next)
            done = terminated or truncated
            ep_ret += r

            remaining = cfg.max_steps - (t + 1)
            # ä¼°è®¡ä» s_next åœ¨å‰©ä½™æ­¥æ•°å†…æˆåŠŸçš„æ¦‚ç‡
            success_prob = estimate_goal_prob_cached(
                mc_env,
                cache,
                s_next,
                remaining,
                policy=policy_fn,
                Q=Q,
                goal_reward=cfg.goal_reward,
                n_rollouts=n_rollouts,
            )
            # æœŸæƒä»·æ ¼: C_{t+1} = e^{-r * remaining} * E^Q[success]
            C_next = (disc ** max(remaining, 0)) * success_prob * cfg.goal_reward

            # TD ç›®æ ‡: å³æ—¶å¥–åŠ± + æœŸæƒä»·æ ¼ï¼ˆæ›¿ä»£ä¼ ç»Ÿ bootstrapï¼‰
            td_target = r + C_next
            Q[s, a] += alpha * (td_target - Q[s, a])

            s = s_next
            t += 1

        returns.append(ep_ret)
        successes.append(1.0 if ep_ret >= cfg.goal_reward else 0.0)
        episodes += 1

    elapsed = time.time() - start
    env.close()
    mc_env.close()
    return np.array(returns), np.array(successes), episodes, elapsed


# =============================================================================
# è¯„ä¼°ä¸æŠ¥å‘Š
# =============================================================================

def smooth(x: np.ndarray, w: int = 50) -> np.ndarray:
    if len(x) < w:
        return x
    return np.convolve(x, np.ones(w) / w, mode="valid")


def compute_metrics(successes: np.ndarray, returns: np.ndarray, episodes: int):
    """è®¡ç®—å…³é”®æŒ‡æ ‡ã€‚"""
    tail_size = min(100, len(successes))
    tail_success_rate = successes[-tail_size:].mean() if tail_size > 0 else 0.0
    overall_success_rate = successes.mean() if len(successes) > 0 else 0.0
    avg_return = returns.mean() if len(returns) > 0 else 0.0
    
    # é¦–æ¬¡æˆåŠŸçš„ episode (å¦‚æœæœ‰)
    first_success = None
    for i, s in enumerate(successes):
        if s > 0:
            first_success = i
            break
    
    return {
        "episodes": episodes,
        "tail_success_rate": tail_success_rate,
        "overall_success_rate": overall_success_rate,
        "avg_return": avg_return,
        "first_success_episode": first_success,
    }


def print_comparison(env_name: str, q_metrics: dict, opt_metrics: dict, 
                     q_time: float, opt_time: float):
    """æ‰“å°å¯¹æ¯”ç»“æœã€‚"""
    print(f"\n{'='*60}")
    print(f" ç¯å¢ƒ: {env_name}")
    print(f"{'='*60}")
    
    print(f"\n{'æŒ‡æ ‡':<25} {'Q-Learning':>15} {'OptionRL':>15}")
    print("-" * 55)
    print(f"{'è®­ç»ƒæ—¶é—´ (s)':<25} {q_time:>15.2f} {opt_time:>15.2f}")
    print(f"{'è®­ç»ƒ Episodes':<25} {q_metrics['episodes']:>15d} {opt_metrics['episodes']:>15d}")
    print(f"{'é¦–æ¬¡æˆåŠŸ Episode':<25} {str(q_metrics['first_success_episode']):>15} {str(opt_metrics['first_success_episode']):>15}")
    print(f"{'æ€»ä½“æˆåŠŸç‡':<25} {q_metrics['overall_success_rate']:>15.3f} {opt_metrics['overall_success_rate']:>15.3f}")
    print(f"{'æœ«å°¾100è½®æˆåŠŸç‡':<25} {q_metrics['tail_success_rate']:>15.3f} {opt_metrics['tail_success_rate']:>15.3f}")
    print(f"{'å¹³å‡å›æŠ¥':<25} {q_metrics['avg_return']:>15.3f} {opt_metrics['avg_return']:>15.3f}")
    
    # æ•ˆç‡æ¯”è¾ƒ
    if q_metrics['tail_success_rate'] > 0 and opt_metrics['tail_success_rate'] > 0:
        ratio = opt_metrics['tail_success_rate'] / q_metrics['tail_success_rate']
        print(f"\nğŸ“Š OptionRL æœ«å°¾æˆåŠŸç‡æ˜¯ Q-Learning çš„ {ratio:.2f}x")
    elif opt_metrics['tail_success_rate'] > 0 and q_metrics['tail_success_rate'] == 0:
        print(f"\nğŸ“Š OptionRL æˆåŠŸå­¦ä¹ ï¼ŒQ-Learning åœ¨è¯¥æ—¶é—´å†…æœªèƒ½å­¦ä¹ åˆ°æœ‰æ•ˆç­–ç•¥")
    elif q_metrics['tail_success_rate'] > 0 and opt_metrics['tail_success_rate'] == 0:
        print(f"\nğŸ“Š Q-Learning æˆåŠŸå­¦ä¹ ï¼ŒOptionRL åœ¨è¯¥æ—¶é—´å†…æœªèƒ½å­¦ä¹ åˆ°æœ‰æ•ˆç­–ç•¥")
    else:
        print(f"\nğŸ“Š ä¸¤ç§æ–¹æ³•åœ¨è¯¥æ—¶é—´é¢„ç®—å†…å‡æœªå­¦ä¹ åˆ°æœ‰æ•ˆç­–ç•¥")


# =============================================================================
# ä¸»å‡½æ•°
# =============================================================================

def run_benchmark(env_key: str, time_budget_s: float = 10.0, n_rollouts: int = 16):
    """åœ¨å•ä¸ªç¯å¢ƒä¸Šè¿è¡ŒåŸºå‡†æµ‹è¯•ã€‚"""
    cfg = ENVS[env_key]
    
    print(f"\nğŸš€ å¼€å§‹æµ‹è¯•: {cfg.name}")
    print(f"   æ—¶é—´é¢„ç®—: {time_budget_s}s (æ¯ç§ç®—æ³•)")
    print(f"   æœ€å¤§æ­¥æ•°/episode: {cfg.max_steps}")
    
    # Q-Learning
    print(f"   [1/2] è¿è¡Œ Q-Learning...")
    ret_q, succ_q, epi_q, t_q = run_q_learning(cfg, time_budget_s=time_budget_s)
    
    # OptionRL
    print(f"   [2/2] è¿è¡Œ OptionRL...")
    ret_opt, succ_opt, epi_opt, t_opt = run_optionrl(
        cfg, time_budget_s=time_budget_s, n_rollouts=n_rollouts
    )
    
    # è®¡ç®—æŒ‡æ ‡
    q_metrics = compute_metrics(succ_q, ret_q, epi_q)
    opt_metrics = compute_metrics(succ_opt, ret_opt, epi_opt)
    
    # æ‰“å°å¯¹æ¯”
    print_comparison(cfg.name, q_metrics, opt_metrics, t_q, t_opt)
    
    return {
        "env": cfg.name,
        "q_learning": q_metrics,
        "optionrl": opt_metrics,
    }


def main():
    print("=" * 60)
    print(" OptionRL vs Q-Learning: ç¨€ç–å¥–åŠ±åŸºå‡†æµ‹è¯•")
    print(" æµ‹è¯•ç›®æ ‡: åœ¨ç›¸åŒæ—¶é—´é¢„ç®—ä¸‹æ¯”è¾ƒä¸¤ç§ç®—æ³•çš„å­¦ä¹ æ•ˆç‡")
    print("=" * 60)
    
    # é…ç½®
    time_budget_s = 10.0  # æ¯ç§ç®—æ³•æ¯ä¸ªç¯å¢ƒçš„æ—¶é—´é¢„ç®—
    n_rollouts = 16       # OptionRL çš„ MC rollout æ•°é‡
    
    results = []
    
    # æµ‹è¯•æ‰€æœ‰ç¯å¢ƒ
    for env_key in ["frozenlake_4x4", "frozenlake_8x8", "taxi"]:
        result = run_benchmark(env_key, time_budget_s=time_budget_s, n_rollouts=n_rollouts)
        results.append(result)
    
    # æ€»ç»“
    print("\n" + "=" * 60)
    print(" ğŸ“‹ æ€»ç»“")
    print("=" * 60)
    print("""
å…³é”®å‘ç°:
1. åœ¨ç®€å•ä»»åŠ¡ (FrozenLake 4x4) ä¸Šï¼ŒQ-Learning èƒ½è·‘æ›´å¤š episodesï¼Œ
   å¯èƒ½å·²ç»è¶³å¤Ÿå­¦åˆ°æœ‰æ•ˆç­–ç•¥ã€‚

2. åœ¨æ›´å¤æ‚ä»»åŠ¡ (FrozenLake 8x8, Taxi-v3) ä¸Šï¼Œç”±äº:
   - çŠ¶æ€ç©ºé—´æ›´å¤§
   - è·¯å¾„æ›´é•¿
   - å¥–åŠ±æ›´ç¨€ç–
   Q-Learning éœ€è¦æ›´å¤šçš„éšæœºæ¢ç´¢æ‰èƒ½"æ’åˆ°"æ­£å¥–åŠ±ï¼Œ
   è€Œ OptionRL é€šè¿‡ä¼°è®¡"æœªæ¥æˆåŠŸæ¦‚ç‡"çš„æœŸæƒä»·æ ¼ C_tï¼Œ
   èƒ½æ›´å¿«åœ°å°†è¿œæœŸå¥–åŠ±ä¿¡å·ä¼ æ’­åˆ°å½“å‰çŠ¶æ€ã€‚

3. OptionRL çš„å• episode è®¡ç®—é‡æ›´å¤§ï¼ˆå› ä¸º MC rolloutï¼‰ï¼Œ
   æ‰€ä»¥åœ¨ç›¸åŒæ—¶é—´å†…è·‘çš„ episodes æ›´å°‘ï¼›
   ä½†æ¯ä¸ª episode çš„ä»·å€¼æ›´æ–°æ›´æœ‰æ–¹å‘æ€§ã€‚

4. è¿™æ˜¯ä¸€ç§"è®¡ç®—æ¢ç»“æ„"çš„ trade-offï¼š
   OptionRL ç”¨æ›´å¤šçš„å•æ­¥è®¡ç®—ï¼Œæ¢å–å¯¹ç¨€ç–å¥–åŠ±æ›´å¥½çš„å¤„ç†èƒ½åŠ›ã€‚
""")


if __name__ == "__main__":
    main()
