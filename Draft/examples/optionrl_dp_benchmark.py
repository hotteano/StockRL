"""
OptionRL vs Q-Learning: å…¬å¹³å¯¹æ¯”ç‰ˆï¼ˆDP é¢„è®¡ç®—æœŸæƒä»·æ ¼ï¼‰

æ ¸å¿ƒæ”¹è¿›:
- ä¸å†ç”¨åœ¨çº¿ MC rollout ä¼°è®¡æˆåŠŸæ¦‚ç‡ï¼ˆå¤ªæ…¢ï¼‰
- æ”¹ç”¨ DP é¢„è®¡ç®—"ä»æ¯ä¸ªçŠ¶æ€å‡ºå‘ã€ç”¨éšæœºç­–ç•¥ã€åˆ°è¾¾ç›®æ ‡çš„æ¦‚ç‡"
- è¿™æ · OptionRL çš„å•æ­¥æˆæœ¬å’Œ Q-learning å‡ ä¹ä¸€æ ·
- æˆ‘ä»¬æ‰èƒ½çœŸæ­£æ¯”è¾ƒ"ç»“æ„åŒ– bootstrap" vs "ä¼ ç»Ÿ TD" çš„æ•ˆæœ

å…³é”®æ´å¯Ÿ:
åœ¨ OptionRL æ¡†æ¶ä¸‹ï¼ŒC_t(s) = e^{-r(T-t)} * E^Q[R_T | s_t = s] æ˜¯ä¸€ä¸ª"æœŸæƒä»·æ ¼"ã€‚
å¯¹äºç¦»æ•£ç¯å¢ƒï¼Œæˆ‘ä»¬å¯ä»¥ç”¨ DP ç²¾ç¡®è®¡ç®—è¿™ä¸ªå€¼ï¼ˆæˆ–ç”¨ç­–ç•¥è¿­ä»£è¿‘ä¼¼ï¼‰ï¼Œ
è€Œä¸éœ€è¦æ¯æ­¥åšæ˜‚è´µçš„ MC rolloutã€‚
"""

import math
import random
import time
from dataclasses import dataclass
from typing import Tuple, Optional
import numpy as np

try:
    import gymnasium as gym
except ImportError:
    print("è¯·å…ˆå®‰è£… gymnasium: pip install gymnasium")
    exit(1)


# =============================================================================
# ç¯å¢ƒé…ç½®
# =============================================================================

@dataclass
class EnvConfig:
    env_id: str
    map_name: Optional[str]
    is_slippery: bool
    max_steps: int
    goal_reward: float
    name: str


ENVS = {
    "frozenlake_4x4": EnvConfig(
        env_id="FrozenLake-v1",
        map_name="4x4",
        is_slippery=True,
        max_steps=100,
        goal_reward=1.0,
        name="FrozenLake 4x4",
    ),
    "frozenlake_8x8": EnvConfig(
        env_id="FrozenLake-v1",
        map_name="8x8",
        is_slippery=True,
        max_steps=200,
        goal_reward=1.0,
        name="FrozenLake 8x8",
    ),
    "taxi": EnvConfig(
        env_id="Taxi-v3",
        map_name=None,
        is_slippery=False,
        max_steps=200,
        goal_reward=20.0,
        name="Taxi-v3",
    ),
}


def make_env(cfg: EnvConfig):
    if cfg.map_name:
        return gym.make(cfg.env_id, map_name=cfg.map_name, is_slippery=cfg.is_slippery)
    return gym.make(cfg.env_id)


# =============================================================================
# DP é¢„è®¡ç®—: ä»æ¯ä¸ªçŠ¶æ€åˆ°è¾¾ç›®æ ‡çš„æ¦‚ç‡
# =============================================================================

def compute_goal_probability_dp(
    env,
    goal_states: set,
    hole_states: set,
    n_iterations: int = 100,
) -> np.ndarray:
    """
    ç”¨åŠ¨æ€è§„åˆ’è®¡ç®—ï¼šä»æ¯ä¸ªçŠ¶æ€å‡ºå‘ï¼Œç”¨å‡åŒ€éšæœºç­–ç•¥ï¼Œæœ€ç»ˆåˆ°è¾¾ç›®æ ‡çš„æ¦‚ç‡ã€‚
    
    è¿™æ˜¯ OptionRL ä¸­ C_t(s) çš„æ ¸å¿ƒï¼šE^Q[success | s_t = s]
    
    å¯¹äº FrozenLake:
    - goal_states: ç›®æ ‡æ ¼å­ï¼ˆå¥–åŠ± 1ï¼‰
    - hole_states: æ´ï¼ˆç»ˆæ­¢ä½†æ— å¥–åŠ±ï¼‰
    - å…¶ä»–çŠ¶æ€: ç»§ç»­
    """
    n_states = env.observation_space.n
    n_actions = env.action_space.n
    
    # P[s][a] = [(prob, next_state, reward, done), ...]
    # ä» gym ç¯å¢ƒä¸­æå–è½¬ç§»æ¦‚ç‡
    P = env.unwrapped.P
    
    # åˆå§‹åŒ–ï¼šç›®æ ‡çŠ¶æ€æ¦‚ç‡ä¸º 1ï¼Œæ´ä¸º 0ï¼Œå…¶ä»–å¾…è®¡ç®—
    V = np.zeros(n_states)
    for g in goal_states:
        V[g] = 1.0
    
    # å€¼è¿­ä»£ï¼šV(s) = (1/n_actions) * sum_a sum_{s'} P(s'|s,a) * V(s')
    for _ in range(n_iterations):
        V_new = np.zeros(n_states)
        for s in range(n_states):
            if s in goal_states:
                V_new[s] = 1.0
                continue
            if s in hole_states:
                V_new[s] = 0.0
                continue
            
            # å‡åŒ€éšæœºç­–ç•¥
            total = 0.0
            for a in range(n_actions):
                for prob, next_state, reward, done in P[s][a]:
                    if done:
                        # ç»ˆæ­¢çŠ¶æ€ï¼šå¦‚æœæ˜¯ç›®æ ‡åˆ™ 1ï¼Œå¦åˆ™ 0
                        total += prob * (1.0 if next_state in goal_states else 0.0) / n_actions
                    else:
                        total += prob * V[next_state] / n_actions
            V_new[s] = total
        V = V_new
    
    return V


def compute_goal_probability_with_policy(
    env,
    Q: np.ndarray,
    goal_states: set,
    hole_states: set,
    eps: float = 0.1,
    n_iterations: int = 50,
) -> np.ndarray:
    """
    ç”¨å½“å‰ Q è¡¨çš„ Îµ-greedy ç­–ç•¥ï¼Œè®¡ç®—ä»æ¯ä¸ªçŠ¶æ€åˆ°è¾¾ç›®æ ‡çš„æ¦‚ç‡ã€‚
    è¿™æ˜¯æ›´ç²¾ç¡®çš„ OptionRLï¼šC_t éšç€ç­–ç•¥æ”¹è¿›è€Œæ›´æ–°ã€‚
    """
    n_states = env.observation_space.n
    n_actions = env.action_space.n
    P = env.unwrapped.P
    
    V = np.zeros(n_states)
    for g in goal_states:
        V[g] = 1.0
    
    for _ in range(n_iterations):
        V_new = np.zeros(n_states)
        for s in range(n_states):
            if s in goal_states:
                V_new[s] = 1.0
                continue
            if s in hole_states:
                V_new[s] = 0.0
                continue
            
            # Îµ-greedy ç­–ç•¥ä¸‹çš„æœŸæœ›
            greedy_a = int(np.argmax(Q[s]))
            total = 0.0
            for a in range(n_actions):
                # ç­–ç•¥æ¦‚ç‡
                if a == greedy_a:
                    pi_a = 1.0 - eps + eps / n_actions
                else:
                    pi_a = eps / n_actions
                
                for prob, next_state, reward, done in P[s][a]:
                    if done:
                        total += pi_a * prob * (1.0 if next_state in goal_states else 0.0)
                    else:
                        total += pi_a * prob * V[next_state]
            V_new[s] = total
        V = V_new
    
    return V


def get_frozenlake_special_states(env) -> Tuple[set, set]:
    """è·å– FrozenLake çš„ç›®æ ‡çŠ¶æ€å’Œæ´çŠ¶æ€ã€‚"""


# =============================================================================
# Taxi-v3 ä¸“ç”¨ DP å‡½æ•°
# =============================================================================

def compute_expected_reward_taxi_dp(
    env,
    gamma: float = 0.99,
    n_iterations: int = 100,
) -> np.ndarray:
    """
    ç”¨ DP è®¡ç®— Taxi-v3 ä¸­ä»æ¯ä¸ªçŠ¶æ€å‡ºå‘çš„æœŸæœ›æŠ˜ç°å›æŠ¥ï¼ˆç”¨éšæœºç­–ç•¥ï¼‰ã€‚
    
    Taxi çš„å¥–åŠ±ç»“æ„ï¼š
    - æˆåŠŸé€è¾¾ä¹˜å®¢: +20
    - éæ³• pickup/dropoff: -10
    - æ¯æ­¥ç§»åŠ¨: -1
    """
    n_states = env.observation_space.n
    n_actions = env.action_space.n
    P = env.unwrapped.P
    
    V = np.zeros(n_states)
    
    for _ in range(n_iterations):
        V_new = np.zeros(n_states)
        for s in range(n_states):
            total = 0.0
            for a in range(n_actions):
                for prob, next_state, reward, done in P[s][a]:
                    if done:
                        total += prob * reward / n_actions
                    else:
                        total += prob * (reward + gamma * V[next_state]) / n_actions
            V_new[s] = total
        V = V_new
    
    return V


def compute_expected_reward_taxi_with_policy(
    env,
    Q: np.ndarray,
    gamma: float = 0.99,
    eps: float = 0.1,
    n_iterations: int = 50,
) -> np.ndarray:
    """
    ç”¨å½“å‰ Q è¡¨çš„ Îµ-greedy ç­–ç•¥ï¼Œè®¡ç®— Taxi ä¸­ä»æ¯ä¸ªçŠ¶æ€å‡ºå‘çš„æœŸæœ›æŠ˜ç°å›æŠ¥ã€‚
    """
    n_states = env.observation_space.n
    n_actions = env.action_space.n
    P = env.unwrapped.P
    
    V = np.zeros(n_states)
    
    for _ in range(n_iterations):
        V_new = np.zeros(n_states)
        for s in range(n_states):
            greedy_a = int(np.argmax(Q[s]))
            total = 0.0
            for a in range(n_actions):
                if a == greedy_a:
                    pi_a = 1.0 - eps + eps / n_actions
                else:
                    pi_a = eps / n_actions
                
                for prob, next_state, reward, done in P[s][a]:
                    if done:
                        total += pi_a * prob * reward
                    else:
                        total += pi_a * prob * (reward + gamma * V[next_state])
            V_new[s] = total
        V = V_new
    
    return V


def get_frozenlake_special_states(env) -> Tuple[set, set]:
    """è·å– FrozenLake çš„ç›®æ ‡çŠ¶æ€å’Œæ´çŠ¶æ€ã€‚"""
    desc = env.unwrapped.desc.flatten()
    goal_states = set()
    hole_states = set()
    for i, cell in enumerate(desc):
        if cell == b'G':
            goal_states.add(i)
        elif cell == b'H':
            hole_states.add(i)
    return goal_states, hole_states


# =============================================================================
# Q-Learning for Taxi
# =============================================================================

def run_q_learning_taxi(
    cfg: EnvConfig,
    n_episodes: int = 10000,
    alpha: float = 0.1,
    gamma: float = 0.99,
    eps: float = 0.1,
) -> Tuple[np.ndarray, np.ndarray, float]:
    """Taxi ä¸“ç”¨ Q-learningï¼ˆåˆ¤æ–­æˆåŠŸç”¨ reward >= goal_rewardï¼‰ã€‚"""
    env = make_env(cfg)
    n_states = env.observation_space.n
    n_actions = env.action_space.n
    Q = np.zeros((n_states, n_actions), dtype=float)
    successes = []

    start = time.time()
    for _ in range(n_episodes):
        obs, _ = env.reset()
        s = int(obs)
        done = False
        ep_ret = 0.0
        steps = 0
        success_this_ep = False
        while not done and steps < cfg.max_steps:
            if random.random() < eps:
                a = random.randint(0, n_actions - 1)
            else:
                a = int(np.argmax(Q[s]))

            obs_next, r, terminated, truncated, _ = env.step(a)
            s_next = int(obs_next)
            done = terminated or truncated
            ep_ret += r
            
            # Taxi æˆåŠŸé€è¾¾ä¹˜å®¢ä¼šå¾—åˆ° +20
            if r >= cfg.goal_reward:
                success_this_ep = True

            td_target = r + (0.0 if done else gamma * np.max(Q[s_next]))
            Q[s, a] += alpha * (td_target - Q[s, a])

            s = s_next
            steps += 1

        successes.append(1.0 if success_this_ep else 0.0)

    elapsed = time.time() - start
    env.close()
    return np.array(successes), Q, elapsed


# =============================================================================
# OptionRL-DP for Taxi
# =============================================================================

def run_optionrl_dp_taxi(
    cfg: EnvConfig,
    n_episodes: int = 10000,
    alpha: float = 0.1,
    gamma: float = 0.99,
    r_rate: float = 0.05,
    eps: float = 0.1,
    update_C_every: int = 50,
    blend_ratio: float = 0.5,
) -> Tuple[np.ndarray, np.ndarray, float]:
    """
    Taxi ä¸“ç”¨ OptionRL-DPã€‚
    
    å¯¹äº Taxiï¼ŒC(s) æ˜¯ä» s å‡ºå‘çš„æœŸæœ›æŠ˜ç°å›æŠ¥ï¼ˆä¸æ˜¯æˆåŠŸæ¦‚ç‡ï¼‰ã€‚
    """
    env = make_env(cfg)
    n_states = env.observation_space.n
    n_actions = env.action_space.n
    Q = np.zeros((n_states, n_actions), dtype=float)
    successes = []
    
    disc = math.exp(-r_rate)
    
    # åˆå§‹ï¼šç”¨éšæœºç­–ç•¥è®¡ç®—æœŸæœ›å›æŠ¥
    C = compute_expected_reward_taxi_dp(env, gamma=gamma)

    start = time.time()
    for ep in range(n_episodes):
        # å®šæœŸæ›´æ–° C
        if ep > 0 and ep % update_C_every == 0:
            C = compute_expected_reward_taxi_with_policy(env, Q, gamma=gamma, eps=eps)
        
        # åŠ¨æ€æ··åˆæ¯”ä¾‹
        current_blend = max(0.1, blend_ratio * (1 - ep / n_episodes))
        
        obs, _ = env.reset()
        s = int(obs)
        done = False
        ep_ret = 0.0
        t = 0
        success_this_ep = False
        while not done and t < cfg.max_steps:
            if random.random() < eps:
                a = random.randint(0, n_actions - 1)
            else:
                a = int(np.argmax(Q[s]))

            obs_next, r, terminated, truncated, _ = env.step(a)
            s_next = int(obs_next)
            done = terminated or truncated
            ep_ret += r
            
            if r >= cfg.goal_reward:
                success_this_ep = True

            # æ··åˆ TD target
            remaining = cfg.max_steps - (t + 1)
            # å¯¹äº Taxiï¼ŒC æœ¬èº«å°±æ˜¯æœŸæœ›å›æŠ¥ï¼Œä¹˜ä»¥æŠ˜ç°å› å­
            C_next = (disc ** max(remaining, 1)) * max(C[s_next], 0)  # æˆªæ–­è´Ÿå€¼
            Q_bootstrap = 0.0 if done else gamma * np.max(Q[s_next])
            
            td_target = r + current_blend * C_next + (1 - current_blend) * Q_bootstrap
            Q[s, a] += alpha * (td_target - Q[s, a])

            s = s_next
            t += 1

        successes.append(1.0 if success_this_ep else 0.0)

    elapsed = time.time() - start
    env.close()
    return np.array(successes), Q, elapsed

def run_q_learning(
    cfg: EnvConfig,
    n_episodes: int = 10000,
    alpha: float = 0.1,
    gamma: float = 0.99,
    eps: float = 0.1,
) -> Tuple[np.ndarray, np.ndarray, float]:
    """æ ‡å‡† Q-learningã€‚"""
    env = make_env(cfg)
    n_states = env.observation_space.n
    n_actions = env.action_space.n
    Q = np.zeros((n_states, n_actions), dtype=float)
    successes = []

    start = time.time()
    for _ in range(n_episodes):
        obs, _ = env.reset()
        s = int(obs)
        done = False
        ep_ret = 0.0
        steps = 0
        while not done and steps < cfg.max_steps:
            if random.random() < eps:
                a = random.randint(0, n_actions - 1)
            else:
                a = int(np.argmax(Q[s]))

            obs_next, r, terminated, truncated, _ = env.step(a)
            s_next = int(obs_next)
            done = terminated or truncated
            ep_ret += r

            td_target = r + (0.0 if done else gamma * np.max(Q[s_next]))
            Q[s, a] += alpha * (td_target - Q[s, a])

            s = s_next
            steps += 1

        successes.append(1.0 if ep_ret >= cfg.goal_reward else 0.0)

    elapsed = time.time() - start
    env.close()
    return np.array(successes), Q, elapsed


# =============================================================================
# OptionRL with DP (Episode é¢„ç®—ç‰ˆ)
# =============================================================================

def run_optionrl_dp(
    cfg: EnvConfig,
    n_episodes: int = 10000,
    alpha: float = 0.1,
    gamma: float = 0.99,  # æ·»åŠ  gamma ç”¨äºæ··åˆæ›´æ–°
    r_rate: float = 0.05,
    eps: float = 0.1,
    update_C_every: int = 50,  # æ›´é¢‘ç¹åœ°æ›´æ–° C
    blend_ratio: float = 0.5,  # C å’Œä¼ ç»Ÿ bootstrap çš„æ··åˆæ¯”ä¾‹
) -> Tuple[np.ndarray, np.ndarray, float]:
    """
    OptionRL with DP-computed option prices (æ”¹è¿›ç‰ˆ).
    
    æ”¹è¿›ç‚¹ï¼š
    1. æ›´é¢‘ç¹åœ°æ›´æ–° Cï¼ˆæ¯ 50 episodeï¼‰
    2. ä½¿ç”¨æ··åˆ TD target: blend_ratio * C(s') + (1-blend_ratio) * Î³*max Q(s',a')
       è¿™æ ·æ—¢åˆ©ç”¨ OptionRL çš„è¿œæœŸä¿¡å·ï¼Œåˆä¿ç•™ Q-learning çš„å±€éƒ¨ä¼˜åŒ–èƒ½åŠ›
    3. éšç€è®­ç»ƒè¿›è¡Œï¼Œé€æ¸é™ä½ blend_ratioï¼Œè®©ç®—æ³•åæœŸæ›´ä¾èµ– Q å€¼
    """
    env = make_env(cfg)
    n_states = env.observation_space.n
    n_actions = env.action_space.n
    Q = np.zeros((n_states, n_actions), dtype=float)
    successes = []
    
    goal_states, hole_states = get_frozenlake_special_states(env)
    disc = math.exp(-r_rate)
    
    # åˆå§‹ï¼šç”¨éšæœºç­–ç•¥è®¡ç®— C
    C = compute_goal_probability_dp(env, goal_states, hole_states) * cfg.goal_reward

    start = time.time()
    for ep in range(n_episodes):
        # å®šæœŸæ›´æ–° Cï¼ˆç”¨å½“å‰ç­–ç•¥ï¼‰
        if ep > 0 and ep % update_C_every == 0:
            prob_V = compute_goal_probability_with_policy(
                env, Q, goal_states, hole_states, eps=eps
            )
            C = prob_V * cfg.goal_reward
        
        # åŠ¨æ€è°ƒæ•´æ··åˆæ¯”ä¾‹ï¼šæ—©æœŸæ›´ä¾èµ– Cï¼ŒåæœŸæ›´ä¾èµ– Q
        # ä» blend_ratio çº¿æ€§è¡°å‡åˆ° 0.1
        current_blend = max(0.1, blend_ratio * (1 - ep / n_episodes))
        
        obs, _ = env.reset()
        s = int(obs)
        done = False
        ep_ret = 0.0
        t = 0
        while not done and t < cfg.max_steps:
            if random.random() < eps:
                a = random.randint(0, n_actions - 1)
            else:
                a = int(np.argmax(Q[s]))

            obs_next, r, terminated, truncated, _ = env.step(a)
            s_next = int(obs_next)
            done = terminated or truncated
            ep_ret += r

            # æ··åˆ TD target
            remaining = cfg.max_steps - (t + 1)
            C_next = (disc ** max(remaining, 1)) * C[s_next]
            Q_bootstrap = 0.0 if done else gamma * np.max(Q[s_next])
            
            # æ··åˆï¼šæ—©æœŸç”¨ C ä¼ æ’­è¿œæœŸä¿¡å·ï¼ŒåæœŸç”¨ Q bootstrap ç²¾ç»†è°ƒä¼˜
            td_target = r + current_blend * C_next + (1 - current_blend) * Q_bootstrap
            Q[s, a] += alpha * (td_target - Q[s, a])

            s = s_next
            t += 1

        successes.append(1.0 if ep_ret >= cfg.goal_reward else 0.0)

    elapsed = time.time() - start
    env.close()
    return np.array(successes), Q, elapsed


# =============================================================================
# è¯„ä¼°ä¸å¯è§†åŒ–
# =============================================================================

def smooth(x: np.ndarray, w: int = 100) -> np.ndarray:
    if len(x) < w:
        return np.cumsum(x) / (np.arange(len(x)) + 1)
    return np.convolve(x, np.ones(w) / w, mode="valid")


def print_results(env_name: str, 
                  succ_q: np.ndarray, t_q: float,
                  succ_opt: np.ndarray, t_opt: float,
                  n_episodes: int):
    """æ‰“å°å¯¹æ¯”ç»“æœã€‚"""
    print(f"\n{'='*65}")
    print(f" ç¯å¢ƒ: {env_name} | Episodes: {n_episodes}")
    print(f"{'='*65}")
    
    # åˆ†æ®µç»Ÿè®¡
    segments = [
        ("å‰ 1000 è½®", 0, 1000),
        ("1000-5000 è½®", 1000, 5000),
        ("5000-10000 è½®", 5000, 10000),
        ("æœ€å 1000 è½®", -1000, None),
    ]
    
    print(f"\n{'é˜¶æ®µ':<20} {'Q-Learning':>15} {'OptionRL-DP':>15}")
    print("-" * 50)
    
    for name, start, end in segments:
        if end is None:
            q_rate = succ_q[start:].mean() if len(succ_q) >= abs(start) else 0
            opt_rate = succ_opt[start:].mean() if len(succ_opt) >= abs(start) else 0
        else:
            q_rate = succ_q[start:end].mean() if len(succ_q) >= end else succ_q[start:].mean()
            opt_rate = succ_opt[start:end].mean() if len(succ_opt) >= end else succ_opt[start:].mean()
        print(f"{name:<20} {q_rate:>15.3f} {opt_rate:>15.3f}")
    
    print("-" * 50)
    print(f"{'è®­ç»ƒæ—¶é—´ (s)':<20} {t_q:>15.2f} {t_opt:>15.2f}")
    print(f"{'æ€»ä½“æˆåŠŸç‡':<20} {succ_q.mean():>15.3f} {succ_opt.mean():>15.3f}")
    
    # é¦–æ¬¡æˆåŠŸ
    first_q = next((i for i, s in enumerate(succ_q) if s > 0), None)
    first_opt = next((i for i, s in enumerate(succ_opt) if s > 0), None)
    print(f"{'é¦–æ¬¡æˆåŠŸ Episode':<20} {str(first_q):>15} {str(first_opt):>15}")
    
    # å­¦ä¹ æ›²çº¿è¶‹åŠ¿
    sm_q = smooth(succ_q)
    sm_opt = smooth(succ_opt)
    if len(sm_q) > 0 and len(sm_opt) > 0:
        print(f"\nğŸ“ˆ å­¦ä¹ æ›²çº¿è¶‹åŠ¿ (å¹³æ»‘å):")
        print(f"   Q-Learning:  {sm_q[0]:.3f} â†’ {sm_q[len(sm_q)//2]:.3f} â†’ {sm_q[-1]:.3f}")
        print(f"   OptionRL-DP: {sm_opt[0]:.3f} â†’ {sm_opt[len(sm_opt)//2]:.3f} â†’ {sm_opt[-1]:.3f}")
    
    # ç»“è®º
    final_q = succ_q[-1000:].mean() if len(succ_q) >= 1000 else succ_q.mean()
    final_opt = succ_opt[-1000:].mean() if len(succ_opt) >= 1000 else succ_opt.mean()
    
    if final_q == 0 and final_opt == 0:
        print(f"\nğŸ”„ ä¸¤è€…æœ€ç»ˆå‡æœªå­¦åˆ°æœ‰æ•ˆç­–ç•¥")
    elif final_opt > final_q * 1.1:
        ratio = final_opt / final_q if final_q > 0 else float('inf')
        print(f"\nâœ… OptionRL-DP åœ¨æœ€åé˜¶æ®µè¡¨ç°ä¼˜äº Q-Learning ({ratio:.2f}x)")
    elif final_q > final_opt * 1.1:
        ratio = final_q / final_opt if final_opt > 0 else float('inf')
        print(f"\nâš ï¸ Q-Learning åœ¨æœ€åé˜¶æ®µè¡¨ç°ä¼˜äº OptionRL-DP ({ratio:.2f}x)")
    else:
        print(f"\nğŸ”„ ä¸¤è€…æœ€ç»ˆè¡¨ç°ç›¸è¿‘")


# =============================================================================
# ä¸»å‡½æ•°
# =============================================================================

def main():
    print("=" * 65)
    print(" OptionRL vs Q-Learning: å…¬å¹³å¯¹æ¯”ç‰ˆ (DP é¢„è®¡ç®—æœŸæƒä»·æ ¼)")
    print(" ç›¸åŒ episode æ•°ï¼Œæ¯”è¾ƒå­¦ä¹ æ›²çº¿å’Œæœ€ç»ˆæ€§èƒ½")
    print("=" * 65)
    
    n_episodes = 10000
    
    for env_key in ["frozenlake_4x4", "frozenlake_8x8"]:
        cfg = ENVS[env_key]
        
        print(f"\nğŸš€ æµ‹è¯•: {cfg.name}")
        print(f"   Episodes: {n_episodes}")
        
        print("   [1/2] è¿è¡Œ Q-Learning...")
        succ_q, Q_q, t_q = run_q_learning(cfg, n_episodes=n_episodes)
        
        print("   [2/2] è¿è¡Œ OptionRL-DP...")
        succ_opt, Q_opt, t_opt = run_optionrl_dp(cfg, n_episodes=n_episodes)
        
        print_results(cfg.name, succ_q, t_q, succ_opt, t_opt, n_episodes)
    
    # Taxi-v3 å•ç‹¬æµ‹è¯•
    print("\n" + "=" * 65)
    print(" ğŸš• Taxi-v3 æµ‹è¯•")
    print("=" * 65)
    
    cfg = ENVS["taxi"]
    print(f"\nğŸš€ æµ‹è¯•: {cfg.name}")
    print(f"   Episodes: {n_episodes}")
    
    print("   [1/2] è¿è¡Œ Q-Learning...")
    succ_q, Q_q, t_q = run_q_learning_taxi(cfg, n_episodes=n_episodes)
    
    print("   [2/2] è¿è¡Œ OptionRL-DP...")
    succ_opt, Q_opt, t_opt = run_optionrl_dp_taxi(cfg, n_episodes=n_episodes)
    
    print_results(cfg.name, succ_q, t_q, succ_opt, t_opt, n_episodes)
    
    print("\n" + "=" * 65)
    print(" ğŸ“‹ å…³é”®æ´å¯Ÿ")
    print("=" * 65)
    print("""
1. DP ç‰ˆ OptionRL çš„å•æ­¥æˆæœ¬å’Œ Q-learning å‡ ä¹ä¸€æ ·ï¼Œ
   æ‰€ä»¥æˆ‘ä»¬ç°åœ¨æ¯”çš„æ˜¯"ç»“æ„"è€Œä¸æ˜¯"è®¡ç®—é‡"ã€‚

2. OptionRL çš„ TD target æ˜¯ r + C(s')ï¼Œå…¶ä¸­ C(s') ç¼–ç äº†
   "ä» s' å‡ºå‘ã€åœ¨å½“å‰ç­–ç•¥ä¸‹ã€æœ€ç»ˆæˆåŠŸçš„æŠ˜ç°æœŸæœ›"ã€‚
   è¿™æ¯” Q-learning çš„ r + Î³*max Q(s',a') æ›´ç›´æ¥åœ°ä¼ æ’­è¿œæœŸå¥–åŠ±ã€‚

3. åœ¨ FrozenLake è¿™ç§ç¯å¢ƒé‡Œï¼ŒOptionRL çš„ä¼˜åŠ¿ä½“ç°åœ¨ï¼š
   - æ›´æ—©çœ‹åˆ°"æœ‰å¸Œæœ›"çš„ä¿¡å·ï¼ˆå³ä½¿è¿˜æ²¡çœŸæ­£æˆåŠŸè¿‡ï¼‰
   - æ›´ç¨³å®šçš„å­¦ä¹ æ›²çº¿ï¼ˆå› ä¸º C æ˜¯å…¨å±€è®¡ç®—çš„ï¼Œä¸ä¾èµ–å±€éƒ¨æ¢ç´¢ï¼‰

4. æ³¨æ„ï¼šå½“å‰å®ç°æ¯ 100 ä¸ª episode é‡æ–°ç”¨ DP è®¡ç®—ä¸€æ¬¡ Cï¼Œ
   ä»¥è®© C è·Ÿéšç­–ç•¥æ”¹è¿›è€Œæ›´æ–°ã€‚è¿™æ˜¯ç†è®ºä¸Šæ›´æ­£ç¡®çš„åšæ³•ã€‚
""")


if __name__ == "__main__":
    main()
