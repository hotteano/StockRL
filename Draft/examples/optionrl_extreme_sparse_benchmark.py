"""
OptionRL vs Q-Learning: æç¨€ç–å¥–åŠ±ä¸“é¡¹æµ‹è¯•

è®¾è®¡ç†å¿µï¼š
- ä¼ ç»Ÿ Q-Learning ä¾èµ– bootstrap é“¾æ¡ï¼šQ(s) <- Q(s') <- Q(s'') <- ... <- reward
- å¦‚æœé“¾æ¡å¤ªé•¿ã€ä¸­é—´æ–­è£‚ï¼ŒQ-Learning å°±å­¦ä¸åˆ°ä¸œè¥¿
- OptionRL ç›´æ¥ä¼°è®¡"ä»å½“å‰çŠ¶æ€åˆ°ç»ˆç‚¹çš„æœŸæœ›"ï¼Œç»•è¿‡ä¸­é—´é“¾æ¡

æœ¬æµ‹è¯•è®¾è®¡äº†å‡ ä¸ª"Q-Learning å‡ ä¹å¿…ç„¶å¤±è´¥"çš„æç«¯ç¨€ç–ç¯å¢ƒï¼š
1. LongChain: ä¸€ç»´é•¿é“¾ï¼Œåªæœ‰ç»ˆç‚¹æœ‰å¥–åŠ±
2. DeepMaze: æ·±åº¦ä¼˜å…ˆçš„è¿·å®«ï¼Œéœ€è¦èµ°å¾ˆé•¿è·¯å¾„
3. NeedleInHaystack: å¤§çŠ¶æ€ç©ºé—´ä¸­åªæœ‰ä¸€ä¸ª"é’ˆ"çŠ¶æ€æœ‰å¥–åŠ±
4. DelayedReward: å¿…é¡»å®Œæˆç‰¹å®šåºåˆ—æ‰èƒ½è·å¾—å¥–åŠ±
"""

import math
import random
import time
from dataclasses import dataclass
from typing import Tuple, Dict, List, Optional
from abc import ABC, abstractmethod
import numpy as np


# =============================================================================
# æŠ½è±¡ç¯å¢ƒåŸºç±»
# =============================================================================

class SparseEnv(ABC):
    """æç¨€ç–å¥–åŠ±ç¯å¢ƒçš„æŠ½è±¡åŸºç±»"""
    
    @abstractmethod
    def reset(self) -> int:
        """é‡ç½®ç¯å¢ƒï¼Œè¿”å›åˆå§‹çŠ¶æ€"""
        pass
    
    @abstractmethod
    def step(self, action: int) -> Tuple[int, float, bool]:
        """æ‰§è¡ŒåŠ¨ä½œï¼Œè¿”å› (next_state, reward, done)"""
        pass
    
    @property
    @abstractmethod
    def n_states(self) -> int:
        pass
    
    @property
    @abstractmethod
    def n_actions(self) -> int:
        pass
    
    @property
    @abstractmethod
    def name(self) -> str:
        pass
    
    @abstractmethod
    def get_transition_probs(self) -> Dict:
        """è¿”å›è½¬ç§»æ¦‚ç‡å­—å…¸ P[s][a] = [(prob, next_s, reward, done), ...]"""
        pass


# =============================================================================
# ç¯å¢ƒ 1: LongChain - ä¸€ç»´é•¿é“¾
# =============================================================================

class LongChainEnv(SparseEnv):
    """
    ä¸€ç»´é•¿é“¾ç¯å¢ƒ:
    
    [0] - [1] - [2] - ... - [N-1] - [GOAL]
    
    - çŠ¶æ€: 0 åˆ° chain_length
    - åŠ¨ä½œ: 0=å·¦, 1=å³
    - å¥–åŠ±: åªæœ‰åˆ°è¾¾ GOAL (state = chain_length) æ‰å¾— +1
    - éš¾ç‚¹: é“¾è¶Šé•¿ï¼ŒQ-Learning çš„ bootstrap é“¾æ¡è¶Šéš¾ä¼ æ’­
    """
    
    def __init__(self, chain_length: int = 20, slip_prob: float = 0.1):
        self.chain_length = chain_length
        self.slip_prob = slip_prob
        self.state = 0
        self.goal_state = chain_length
        self._n_states = chain_length + 1
        self._n_actions = 2  # 0=å·¦, 1=å³
    
    @property
    def n_states(self) -> int:
        return self._n_states
    
    @property
    def n_actions(self) -> int:
        return self._n_actions
    
    @property
    def name(self) -> str:
        return f"LongChain-{self.chain_length}"
    
    def reset(self) -> int:
        self.state = 0
        return self.state
    
    def step(self, action: int) -> Tuple[int, float, bool]:
        # æ»‘åŠ¨æ¦‚ç‡ï¼šæœ‰æ—¶å€™åŠ¨ä½œä¼šè¢«"ç¿»è½¬"
        if random.random() < self.slip_prob:
            action = 1 - action
        
        if action == 1:  # å³
            self.state = min(self.state + 1, self.goal_state)
        else:  # å·¦
            self.state = max(self.state - 1, 0)
        
        # åªæœ‰åˆ°è¾¾ç»ˆç‚¹æ‰æœ‰å¥–åŠ±
        if self.state == self.goal_state:
            return self.state, 1.0, True
        return self.state, 0.0, False
    
    def get_transition_probs(self) -> Dict:
        P = {}
        for s in range(self._n_states):
            P[s] = {}
            for a in range(self._n_actions):
                transitions = []
                
                if s == self.goal_state:
                    # ç»ˆç‚¹çŠ¶æ€ï¼šä¿æŒä¸åŠ¨
                    transitions.append((1.0, s, 0.0, True))
                else:
                    # æ­£å¸¸åŠ¨ä½œ
                    for actual_a, prob in [(a, 1 - self.slip_prob), (1 - a, self.slip_prob)]:
                        if actual_a == 1:  # å³
                            next_s = min(s + 1, self.goal_state)
                        else:  # å·¦
                            next_s = max(s - 1, 0)
                        
                        reward = 1.0 if next_s == self.goal_state else 0.0
                        done = next_s == self.goal_state
                        transitions.append((prob, next_s, reward, done))
                
                P[s][a] = transitions
        return P


# =============================================================================
# ç¯å¢ƒ 2: GridMaze - ç½‘æ ¼è¿·å®«ï¼ˆåªæœ‰ä¸€ä¸ªå‡ºå£æœ‰å¥–åŠ±ï¼‰
# =============================================================================

class GridMazeEnv(SparseEnv):
    """
    N x N ç½‘æ ¼è¿·å®«:
    
    - èµ·ç‚¹: (0, 0) å·¦ä¸Šè§’
    - ç»ˆç‚¹: (N-1, N-1) å³ä¸‹è§’
    - åŠ¨ä½œ: 0=ä¸Š, 1=ä¸‹, 2=å·¦, 3=å³
    - å¥–åŠ±: åªæœ‰åˆ°è¾¾ç»ˆç‚¹æ‰å¾— +1
    - æœ‰éšæœºå¢™å£é˜»æŒ¡
    """
    
    def __init__(self, size: int = 8, wall_prob: float = 0.2, slip_prob: float = 0.1):
        self.size = size
        self.slip_prob = slip_prob
        self._n_states = size * size
        self._n_actions = 4  # ä¸Šä¸‹å·¦å³
        
        # ç”Ÿæˆå›ºå®šçš„å¢™å£ï¼ˆç”¨ç§å­ä¿è¯å¯å¤ç°ï¼‰
        rng = np.random.RandomState(42)
        self.walls = set()
        for i in range(size):
            for j in range(size):
                if (i, j) != (0, 0) and (i, j) != (size-1, size-1):
                    if rng.random() < wall_prob:
                        self.walls.add((i, j))
        
        self.state = 0
        self.goal_state = size * size - 1
    
    def _pos_to_state(self, row: int, col: int) -> int:
        return row * self.size + col
    
    def _state_to_pos(self, state: int) -> Tuple[int, int]:
        return state // self.size, state % self.size
    
    @property
    def n_states(self) -> int:
        return self._n_states
    
    @property
    def n_actions(self) -> int:
        return self._n_actions
    
    @property
    def name(self) -> str:
        return f"GridMaze-{self.size}x{self.size}"
    
    def reset(self) -> int:
        self.state = 0
        return self.state
    
    def _move(self, state: int, action: int) -> int:
        row, col = self._state_to_pos(state)
        
        # åŠ¨ä½œæ•ˆæœ
        if action == 0:  # ä¸Š
            new_row, new_col = row - 1, col
        elif action == 1:  # ä¸‹
            new_row, new_col = row + 1, col
        elif action == 2:  # å·¦
            new_row, new_col = row, col - 1
        else:  # å³
            new_row, new_col = row, col + 1
        
        # è¾¹ç•Œæ£€æŸ¥
        if new_row < 0 or new_row >= self.size or new_col < 0 or new_col >= self.size:
            return state
        
        # å¢™å£æ£€æŸ¥
        if (new_row, new_col) in self.walls:
            return state
        
        return self._pos_to_state(new_row, new_col)
    
    def step(self, action: int) -> Tuple[int, float, bool]:
        # æ»‘åŠ¨ï¼šæœ‰æ¦‚ç‡æ‰§è¡ŒéšæœºåŠ¨ä½œ
        if random.random() < self.slip_prob:
            action = random.randint(0, 3)
        
        self.state = self._move(self.state, action)
        
        if self.state == self.goal_state:
            return self.state, 1.0, True
        return self.state, 0.0, False
    
    def get_transition_probs(self) -> Dict:
        P = {}
        for s in range(self._n_states):
            P[s] = {}
            for a in range(self._n_actions):
                transitions = []
                
                if s == self.goal_state:
                    transitions.append((1.0, s, 0.0, True))
                else:
                    # æ­£å¸¸åŠ¨ä½œ
                    next_s_intended = self._move(s, a)
                    reward_intended = 1.0 if next_s_intended == self.goal_state else 0.0
                    done_intended = next_s_intended == self.goal_state
                    transitions.append((1 - self.slip_prob, next_s_intended, reward_intended, done_intended))
                    
                    # æ»‘åŠ¨åˆ°éšæœºåŠ¨ä½œ
                    for rand_a in range(4):
                        next_s_rand = self._move(s, rand_a)
                        reward_rand = 1.0 if next_s_rand == self.goal_state else 0.0
                        done_rand = next_s_rand == self.goal_state
                        transitions.append((self.slip_prob / 4, next_s_rand, reward_rand, done_rand))
                
                P[s][a] = transitions
        return P


# =============================================================================
# ç¯å¢ƒ 3: NeedleInHaystack - å¤§æµ·æé’ˆ
# =============================================================================

class NeedleInHaystackEnv(SparseEnv):
    """
    å¤§æµ·æé’ˆç¯å¢ƒ:
    
    - N ä¸ªçŠ¶æ€ï¼Œéšæœºè¿æ¥
    - åªæœ‰ä¸€ä¸ª"é’ˆ"çŠ¶æ€æœ‰å¥–åŠ±
    - ä»ä»»æ„çŠ¶æ€å¯ä»¥è·³åˆ°è‹¥å¹²ä¸ªé‚»å±…çŠ¶æ€
    - æéš¾é€šè¿‡éšæœºæ¢ç´¢æ‰¾åˆ°é’ˆ
    """
    
    def __init__(self, n_states: int = 100, n_neighbors: int = 4, slip_prob: float = 0.1):
        self._n_states = n_states
        self._n_actions = n_neighbors
        self.slip_prob = slip_prob
        
        # å›ºå®šéšæœºç§å­ç”Ÿæˆå›¾ç»“æ„
        rng = np.random.RandomState(123)
        
        # æ¯ä¸ªçŠ¶æ€æœ‰ n_neighbors ä¸ªé‚»å±…
        self.neighbors = {}
        for s in range(n_states):
            self.neighbors[s] = rng.choice(n_states, size=n_neighbors, replace=False).tolist()
        
        # é’ˆçŠ¶æ€ï¼ˆç›®æ ‡ï¼‰
        self.needle_state = n_states - 1
        self.state = 0
    
    @property
    def n_states(self) -> int:
        return self._n_states
    
    @property
    def n_actions(self) -> int:
        return self._n_actions
    
    @property
    def name(self) -> str:
        return f"NeedleInHaystack-{self._n_states}"
    
    def reset(self) -> int:
        self.state = 0
        return self.state
    
    def step(self, action: int) -> Tuple[int, float, bool]:
        if random.random() < self.slip_prob:
            action = random.randint(0, self._n_actions - 1)
        
        self.state = self.neighbors[self.state][action]
        
        if self.state == self.needle_state:
            return self.state, 1.0, True
        return self.state, 0.0, False
    
    def get_transition_probs(self) -> Dict:
        P = {}
        for s in range(self._n_states):
            P[s] = {}
            for a in range(self._n_actions):
                transitions = []
                
                if s == self.needle_state:
                    transitions.append((1.0, s, 0.0, True))
                else:
                    # æ­£å¸¸åŠ¨ä½œ
                    next_s = self.neighbors[s][a]
                    reward = 1.0 if next_s == self.needle_state else 0.0
                    done = next_s == self.needle_state
                    transitions.append((1 - self.slip_prob, next_s, reward, done))
                    
                    # æ»‘åŠ¨
                    for rand_a in range(self._n_actions):
                        next_s_rand = self.neighbors[s][rand_a]
                        reward_rand = 1.0 if next_s_rand == self.needle_state else 0.0
                        done_rand = next_s_rand == self.needle_state
                        transitions.append((self.slip_prob / self._n_actions, next_s_rand, reward_rand, done_rand))
                
                P[s][a] = transitions
        return P


# =============================================================================
# ç¯å¢ƒ 4: SequenceMatch - å¿…é¡»æŒ‰ç‰¹å®šåºåˆ—è¡ŒåŠ¨
# =============================================================================

class SequenceMatchEnv(SparseEnv):
    """
    åºåˆ—åŒ¹é…ç¯å¢ƒ:
    
    - å¿…é¡»æŒ‰æ­£ç¡®çš„åŠ¨ä½œåºåˆ—è¡ŒåŠ¨æ‰èƒ½è·å¾—å¥–åŠ±
    - ä¾‹å¦‚ï¼šå¿…é¡»ä¾æ¬¡æ‰§è¡Œ [0, 1, 0, 1, 1] æ‰èƒ½æˆåŠŸ
    - ä»»ä½•é”™è¯¯éƒ½ä¼šé‡ç½®è¿›åº¦
    - è¿™æ˜¯æœ€æç«¯çš„ç¨€ç–å¥–åŠ±ï¼šåªæœ‰ä¸€æ¡æ­£ç¡®è·¯å¾„
    """
    
    def __init__(self, sequence_length: int = 8, n_actions: int = 2):
        self.target_sequence = [i % n_actions for i in range(sequence_length)]  # äº¤æ›¿åºåˆ—
        self.sequence_length = sequence_length
        self._n_actions = n_actions
        self._n_states = sequence_length + 1  # è¿›åº¦ 0 åˆ° sequence_length
        self.state = 0  # å½“å‰åŒ¹é…è¿›åº¦
    
    @property
    def n_states(self) -> int:
        return self._n_states
    
    @property
    def n_actions(self) -> int:
        return self._n_actions
    
    @property
    def name(self) -> str:
        return f"SequenceMatch-{self.sequence_length}"
    
    def reset(self) -> int:
        self.state = 0
        return self.state
    
    def step(self, action: int) -> Tuple[int, float, bool]:
        if self.state < self.sequence_length:
            if action == self.target_sequence[self.state]:
                self.state += 1  # åŒ¹é…æˆåŠŸï¼Œè¿›åº¦+1
            else:
                self.state = 0  # åŒ¹é…å¤±è´¥ï¼Œé‡ç½®è¿›åº¦
        
        if self.state == self.sequence_length:
            return self.state, 1.0, True
        return self.state, 0.0, False
    
    def get_transition_probs(self) -> Dict:
        P = {}
        for s in range(self._n_states):
            P[s] = {}
            for a in range(self._n_actions):
                if s == self.sequence_length:
                    # å·²å®Œæˆ
                    P[s][a] = [(1.0, s, 0.0, True)]
                elif a == self.target_sequence[s]:
                    # æ­£ç¡®åŠ¨ä½œ
                    next_s = s + 1
                    reward = 1.0 if next_s == self.sequence_length else 0.0
                    done = next_s == self.sequence_length
                    P[s][a] = [(1.0, next_s, reward, done)]
                else:
                    # é”™è¯¯åŠ¨ä½œ
                    P[s][a] = [(1.0, 0, 0.0, False)]
        return P


# =============================================================================
# DP è®¡ç®—æœŸæœ›æˆåŠŸæ¦‚ç‡/å›æŠ¥
# =============================================================================

def compute_success_prob_dp(env: SparseEnv, n_iterations: int = 200) -> np.ndarray:
    """ç”¨ DP è®¡ç®—ä»æ¯ä¸ªçŠ¶æ€å‡ºå‘ï¼ˆéšæœºç­–ç•¥ï¼‰åˆ°è¾¾ç›®æ ‡çš„æ¦‚ç‡ã€‚"""
    P = env.get_transition_probs()
    n_states = env.n_states
    n_actions = env.n_actions
    
    V = np.zeros(n_states)
    
    for _ in range(n_iterations):
        V_new = np.zeros(n_states)
        for s in range(n_states):
            total = 0.0
            for a in range(n_actions):
                for prob, next_s, reward, done in P[s][a]:
                    if reward > 0:
                        total += prob * 1.0 / n_actions
                    elif not done:
                        total += prob * V[next_s] / n_actions
            V_new[s] = total
        
        if np.allclose(V, V_new, atol=1e-8):
            break
        V = V_new
    
    return V


def compute_success_prob_with_policy(
    env: SparseEnv,
    Q: np.ndarray,
    eps: float = 0.1,
    n_iterations: int = 100,
) -> np.ndarray:
    """ç”¨å½“å‰ Q è¡¨çš„ Îµ-greedy ç­–ç•¥è®¡ç®—æˆåŠŸæ¦‚ç‡ã€‚"""
    P = env.get_transition_probs()
    n_states = env.n_states
    n_actions = env.n_actions
    
    V = np.zeros(n_states)
    
    for _ in range(n_iterations):
        V_new = np.zeros(n_states)
        for s in range(n_states):
            greedy_a = int(np.argmax(Q[s]))
            total = 0.0
            for a in range(n_actions):
                if a == greedy_a:
                    pi_a = 1.0 - eps + eps / n_actions
                else:
                    pi_a = eps / n_actions
                
                for prob, next_s, reward, done in P[s][a]:
                    if reward > 0:
                        total += pi_a * prob * 1.0
                    elif not done:
                        total += pi_a * prob * V[next_s]
            V_new[s] = total
        
        if np.allclose(V, V_new, atol=1e-8):
            break
        V = V_new
    
    return V


# =============================================================================
# Q-Learning
# =============================================================================

def run_q_learning(
    env: SparseEnv,
    n_episodes: int = 10000,
    max_steps: int = 200,
    alpha: float = 0.1,
    gamma: float = 0.99,
    eps: float = 0.1,
) -> Tuple[np.ndarray, float]:
    """æ ‡å‡† Q-learningã€‚"""
    Q = np.zeros((env.n_states, env.n_actions), dtype=float)
    successes = []

    start = time.time()
    for _ in range(n_episodes):
        s = env.reset()
        done = False
        steps = 0
        success = False
        
        while not done and steps < max_steps:
            if random.random() < eps:
                a = random.randint(0, env.n_actions - 1)
            else:
                a = int(np.argmax(Q[s]))

            next_s, r, done = env.step(a)
            
            if r > 0:
                success = True

            td_target = r + (0.0 if done else gamma * np.max(Q[next_s]))
            Q[s, a] += alpha * (td_target - Q[s, a])

            s = next_s
            steps += 1

        successes.append(1.0 if success else 0.0)

    elapsed = time.time() - start
    return np.array(successes), elapsed


# =============================================================================
# OptionRL-DP
# =============================================================================

def run_optionrl_dp(
    env: SparseEnv,
    n_episodes: int = 10000,
    max_steps: int = 200,
    alpha: float = 0.1,
    gamma: float = 0.99,
    r_rate: float = 0.05,
    eps: float = 0.1,
    update_C_every: int = 50,
    blend_ratio: float = 0.5,
) -> Tuple[np.ndarray, float]:
    """OptionRL with DP-computed option pricesã€‚"""
    Q = np.zeros((env.n_states, env.n_actions), dtype=float)
    successes = []
    
    disc = math.exp(-r_rate)
    
    # åˆå§‹ C
    C = compute_success_prob_dp(env)

    start = time.time()
    for ep in range(n_episodes):
        # å®šæœŸæ›´æ–° C
        if ep > 0 and ep % update_C_every == 0:
            C = compute_success_prob_with_policy(env, Q, eps=eps)
        
        # åŠ¨æ€æ··åˆ
        current_blend = max(0.1, blend_ratio * (1 - ep / n_episodes))
        
        s = env.reset()
        done = False
        t = 0
        success = False
        
        while not done and t < max_steps:
            if random.random() < eps:
                a = random.randint(0, env.n_actions - 1)
            else:
                a = int(np.argmax(Q[s]))

            next_s, r, done = env.step(a)
            
            if r > 0:
                success = True

            # æ··åˆ TD target
            remaining = max_steps - (t + 1)
            C_next = (disc ** max(remaining, 1)) * C[next_s]
            Q_bootstrap = 0.0 if done else gamma * np.max(Q[next_s])
            
            td_target = r + current_blend * C_next + (1 - current_blend) * Q_bootstrap
            Q[s, a] += alpha * (td_target - Q[s, a])

            s = next_s
            t += 1

        successes.append(1.0 if success else 0.0)

    elapsed = time.time() - start
    return np.array(successes), elapsed


# =============================================================================
# è¯„ä¼°
# =============================================================================

def smooth(x: np.ndarray, w: int = 100) -> np.ndarray:
    if len(x) < w:
        return np.cumsum(x) / (np.arange(len(x)) + 1)
    return np.convolve(x, np.ones(w) / w, mode="valid")


def first_success_episode(successes: np.ndarray) -> Optional[int]:
    for i, s in enumerate(successes):
        if s > 0:
            return i
    return None


def print_results(env_name: str, succ_q: np.ndarray, t_q: float,
                  succ_opt: np.ndarray, t_opt: float, n_episodes: int):
    """æ‰“å°å¯¹æ¯”ç»“æœã€‚"""
    print(f"\n{'='*70}")
    print(f" ğŸ¯ {env_name}")
    print(f"{'='*70}")
    
    # åˆ†æ®µç»Ÿè®¡
    segments = [
        ("å‰ 20%", 0, n_episodes // 5),
        ("20-50%", n_episodes // 5, n_episodes // 2),
        ("50-80%", n_episodes // 2, n_episodes * 4 // 5),
        ("æœ€å 20%", n_episodes * 4 // 5, n_episodes),
    ]
    
    print(f"\n{'é˜¶æ®µ':<15} {'Q-Learning':>15} {'OptionRL-DP':>15} {'å·®å¼‚':>15}")
    print("-" * 60)
    
    for name, start, end in segments:
        q_rate = succ_q[start:end].mean()
        opt_rate = succ_opt[start:end].mean()
        if q_rate > 0:
            diff = f"{opt_rate/q_rate:.2f}x"
        elif opt_rate > 0:
            diff = "âˆx better"
        else:
            diff = "both 0"
        print(f"{name:<15} {q_rate:>15.4f} {opt_rate:>15.4f} {diff:>15}")
    
    print("-" * 60)
    
    first_q = first_success_episode(succ_q)
    first_opt = first_success_episode(succ_opt)
    
    print(f"{'é¦–æ¬¡æˆåŠŸ':<15} {str(first_q):>15} {str(first_opt):>15}", end="")
    if first_q is None and first_opt is not None:
        print(f" {'OptionRL wins':>15}")
    elif first_q is not None and first_opt is None:
        print(f" {'Q-Learning wins':>15}")
    elif first_q is not None and first_opt is not None:
        ratio = first_q / first_opt if first_opt > 0 else float('inf')
        print(f" {f'{ratio:.1f}x faster':>15}")
    else:
        print(f" {'both failed':>15}")
    
    print(f"{'æ€»æˆåŠŸç‡':<15} {succ_q.mean():>15.4f} {succ_opt.mean():>15.4f}")
    print(f"{'è®­ç»ƒæ—¶é—´':<15} {t_q:>15.2f}s {t_opt:>15.2f}s")
    
    # æœ€ç»ˆåˆ¤å®š
    final_q = succ_q[-n_episodes//5:].mean()
    final_opt = succ_opt[-n_episodes//5:].mean()
    
    print("\nğŸ“Š ç»“è®º: ", end="")
    if final_q == 0 and final_opt == 0:
        print("ä¸¤è€…å‡æœªå­¦åˆ°æœ‰æ•ˆç­–ç•¥")
    elif final_q == 0 and final_opt > 0:
        print(f"âœ… OptionRL æˆåŠŸ ({final_opt:.2%})ï¼ŒQ-Learning å®Œå…¨å¤±è´¥")
    elif final_q > 0 and final_opt == 0:
        print(f"âš ï¸ Q-Learning æˆåŠŸ ({final_q:.2%})ï¼ŒOptionRL å®Œå…¨å¤±è´¥")
    elif final_opt > final_q * 1.5:
        print(f"âœ… OptionRL æ˜¾è‘—æ›´å¥½ ({final_opt:.2%} vs {final_q:.2%})")
    elif final_q > final_opt * 1.5:
        print(f"âš ï¸ Q-Learning æ˜¾è‘—æ›´å¥½ ({final_q:.2%} vs {final_opt:.2%})")
    else:
        print(f"ğŸ”„ ä¸¤è€…è¡¨ç°ç›¸è¿‘ ({final_opt:.2%} vs {final_q:.2%})")


# =============================================================================
# ä¸»å‡½æ•°
# =============================================================================

def main():
    print("=" * 70)
    print(" ğŸ”¬ OptionRL vs Q-Learning: æç¨€ç–å¥–åŠ±ä¸“é¡¹æµ‹è¯•")
    print("=" * 70)
    print("""
æµ‹è¯•ç›®çš„: éªŒè¯ OptionRL åœ¨"Q-Learning å‡ ä¹å¿…ç„¶å¤±è´¥"çš„æç«¯ç¨€ç–ç¯å¢ƒä¸­çš„ä¼˜åŠ¿

ç¯å¢ƒè®¾è®¡:
1. LongChain: ä¸€ç»´é•¿é“¾ï¼Œåªæœ‰ç»ˆç‚¹æœ‰å¥–åŠ±ï¼ˆæµ‹è¯• bootstrap é“¾æ¡é•¿åº¦ï¼‰
2. GridMaze: ç½‘æ ¼è¿·å®«ï¼Œåªæœ‰å‡ºå£æœ‰å¥–åŠ±ï¼ˆæµ‹è¯•çŠ¶æ€ç©ºé—´ + è·¯å¾„é•¿åº¦ï¼‰
3. NeedleInHaystack: å¤§æµ·æé’ˆï¼Œéšæœºå›¾ä¸­æ‰¾å”¯ä¸€ç›®æ ‡ï¼ˆæµ‹è¯•æ¢ç´¢éš¾åº¦ï¼‰
4. SequenceMatch: å¿…é¡»æŒ‰ç‰¹å®šåºåˆ—è¡ŒåŠ¨ï¼ˆæµ‹è¯•æç«¯ç¨€ç–æ€§ï¼‰
""")
    
    n_episodes = 10000
    max_steps = 300
    
    # ç¯å¢ƒé…ç½®
    envs = [
        LongChainEnv(chain_length=30, slip_prob=0.1),
        LongChainEnv(chain_length=50, slip_prob=0.1),
        GridMazeEnv(size=10, wall_prob=0.2, slip_prob=0.1),
        NeedleInHaystackEnv(n_states=100, n_neighbors=4, slip_prob=0.1),
        NeedleInHaystackEnv(n_states=200, n_neighbors=4, slip_prob=0.1),
        SequenceMatchEnv(sequence_length=6, n_actions=2),
        SequenceMatchEnv(sequence_length=8, n_actions=2),
    ]
    
    results = []
    
    for env in envs:
        print(f"\nğŸš€ æµ‹è¯•: {env.name} ({env.n_states} states, {env.n_actions} actions)")
        
        print("   [1/2] Q-Learning...")
        succ_q, t_q = run_q_learning(env, n_episodes=n_episodes, max_steps=max_steps)
        
        print("   [2/2] OptionRL-DP...")
        succ_opt, t_opt = run_optionrl_dp(env, n_episodes=n_episodes, max_steps=max_steps)
        
        print_results(env.name, succ_q, t_q, succ_opt, t_opt, n_episodes)
        
        results.append({
            "env": env.name,
            "q_final": succ_q[-n_episodes//5:].mean(),
            "opt_final": succ_opt[-n_episodes//5:].mean(),
            "q_first": first_success_episode(succ_q),
            "opt_first": first_success_episode(succ_opt),
        })
    
    # æ±‡æ€»
    print("\n" + "=" * 70)
    print(" ğŸ“‹ æ±‡æ€»è¡¨")
    print("=" * 70)
    print(f"\n{'ç¯å¢ƒ':<25} {'Q-Learnæœ€å20%':>15} {'OptionRLæœ€å20%':>15} {'é¦–æ¬¡æˆåŠŸ(Q/Opt)':>20}")
    print("-" * 75)
    for r in results:
        first_str = f"{r['q_first']}/{r['opt_first']}"
        print(f"{r['env']:<25} {r['q_final']:>15.4f} {r['opt_final']:>15.4f} {first_str:>20}")
    
    print("\n" + "=" * 70)
    print(" ğŸ’¡ å…³é”®æ´å¯Ÿ")
    print("=" * 70)
    print("""
1. åœ¨æç¨€ç–å¥–åŠ±ç¯å¢ƒä¸­ï¼ŒQ-Learning çš„ bootstrap é“¾æ¡éš¾ä»¥å»ºç«‹ï¼š
   - éœ€è¦å…ˆ"ç¢°å·§"åˆ°è¾¾ç›®æ ‡çŠ¶æ€ï¼Œæ‰èƒ½å¼€å§‹åå‘ä¼ æ’­ä»·å€¼
   - é“¾æ¡è¶Šé•¿ã€çŠ¶æ€ç©ºé—´è¶Šå¤§ï¼Œè¿™ä¸ªæ¦‚ç‡è¶Šä½

2. OptionRL é€šè¿‡ DP é¢„è®¡ç®—"ä»æ¯ä¸ªçŠ¶æ€åˆ°è¾¾ç›®æ ‡çš„æ¦‚ç‡"C(s)ï¼š
   - å³ä½¿ä»æœªå®é™…åˆ°è¾¾è¿‡ç›®æ ‡ï¼Œä¹Ÿèƒ½ä¼°è®¡æ¯ä¸ªçŠ¶æ€çš„"å¸Œæœ›ç¨‹åº¦"
   - TD target = r + C(s') è®©æ¯ä¸ªçŠ¶æ€éƒ½èƒ½ç«‹å³è·å¾—æœ‰æ„ä¹‰çš„æ›´æ–°ä¿¡å·

3. è¿™æ­£æ˜¯ OptionRL è®ºæ–‡çš„æ ¸å¿ƒ claimï¼š
   æœŸæƒä»·æ ¼ C_t(s) = e^{-r(T-t)} * E^Q[R_T | s_t = s]
   ç¼–ç äº†"è¿œæœŸä»·å€¼çš„ç»“æ„åŒ–å…ˆéªŒ"ï¼Œç»•è¿‡äº†ä¼ ç»Ÿ TD çš„ bootstrap å›°å¢ƒã€‚

4. åœ¨ç®€å•ä»»åŠ¡ä¸Šï¼ŒQ-Learning å¯èƒ½æ›´é«˜æ•ˆï¼ˆä¸éœ€è¦ DP å¼€é”€ï¼‰ï¼›
   ä½†åœ¨æç¨€ç–ä»»åŠ¡ä¸Šï¼ŒOptionRL æ˜¯"èƒ½å­¦åˆ° vs å­¦ä¸åˆ°"çš„æœ¬è´¨å·®å¼‚ã€‚
""")


if __name__ == "__main__":
    main()
